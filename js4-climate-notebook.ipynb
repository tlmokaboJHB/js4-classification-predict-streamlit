{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8d2b84c",
   "metadata": {},
   "source": [
    "# CLASSIFICATION Predict Student Solution\n",
    "\n",
    "© Explore Data Science Academy\n",
    "\n",
    "---\n",
    "### Honour Code\n",
    "\n",
    "We Team JS4, confirm - by submitting this document - that the solutions in this notebook are a result of my own work and that I abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
    "\n",
    "Non-compliance with the honour code constitutes a material breach of contract.\n",
    "\n",
    "### Predict Overview: EDSA - Climate Change Belief Analysis 2022\n",
    "\n",
    "Many companies are built around lessening one’s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received.\n",
    "\n",
    "With this context, EDSA is challenging you during the Classification Sprint with the task of creating a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.\n",
    "\n",
    "Providing an accurate and robust solution to this task gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies.\n",
    "\n",
    "### Data overview\n",
    "Data The collection of this data was funded by a Canada Foundation for Innovation JELF Grant to Chris Bauch, University of Waterloo. The dataset aggregates tweets pertaining to climate change collected between Apr 27, 2015 and Feb 21, 2018. In total, 43943 tweets were collected. Each tweet is labelled as one of the following classes:\n",
    "\n",
    "Class Description\n",
    "* 2 News: the tweet links to factual news about climate change\n",
    "* 1 Pro: the tweet supports the belief of man-made climate change\n",
    "* 0 Neutral: the tweet neither supports nor refutes the belief of man-made climate change\n",
    "* -1 Anti: the tweet does not believe in man-made climate change\n",
    "\n",
    "Variable definitions\n",
    "- sentiment: Sentiment of tweet\n",
    "- message: Tweet body\n",
    "- tweetid: Twitter unique id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a12f436",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# TEAM JS4 MEMBERS\n",
    " 1. Annah Majadibodu\n",
    " 2. Kopano Motalane\n",
    " 3. Lungelo Ndlovu\n",
    " 4. Ouma Nyasengo\n",
    " 5. Phothuma Mavuso\n",
    " 6. Tlotlo Mokabo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ded8e9",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Data Engineering</a>\n",
    "\n",
    "<a href=#four>4. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#five>5. Modeling</a>\n",
    "\n",
    "<a href=#six>6. Model Performance</a>\n",
    "\n",
    "<a href=#seven>7. Model Explanations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438dc4e6",
   "metadata": {},
   "source": [
    "<a id=\"one\"></a>\n",
    "## 1. Importing Packages\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Importing Packages ⚡ |\n",
    "| :--------------------------- |\n",
    "|  |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78070faf",
   "metadata": {},
   "source": [
    "# Import all the  **libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3812352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tlotl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tlotl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data Analysis and Wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualisations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocessing\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "# Modelling\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Model Evaluation Metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "nltk.download(['punkt','stopwords'])\n",
    "%matplotlib inline\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa75f48",
   "metadata": {},
   "source": [
    "### Download NLTK Corpora\n",
    "Some of the `nltk` text processing methods introduced in this train involve a lookup operation. For example, to find all [stopwords](https://www.geeksforgeeks.org/removing-stop-words-nltk-python/) in a given string of text, we require a list of all possible stopwords in the English language to use for the lookup. Such a list is refered to as a [corpus](https://en.wikipedia.org/wiki/Text_corpus). Therefore, we need to first download the corpora we're going use, otherwise we may get a lookup error! Watch out specifically for the `tokenize` and `stopwords` sections. Not to worry, as we can easily avoid these errors by downloading the [corpora](http://www.nltk.org/nltk_data/) using the `nltk` downloader tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92e5d754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b20bef0",
   "metadata": {},
   "source": [
    "pip intall comet version control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96aabdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting comet_ml\n",
      "  Downloading comet_ml-3.31.4-py2.py3-none-any.whl (356 kB)\n",
      "Collecting semantic-version>=2.8.0\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting sentry-sdk>=1.1.0\n",
      "  Downloading sentry_sdk-1.5.12-py2.py3-none-any.whl (145 kB)\n",
      "Collecting websocket-client>=0.55.0\n",
      "  Downloading websocket_client-1.3.3-py3-none-any.whl (54 kB)\n",
      "Requirement already satisfied: six in c:\\users\\tlotl\\anaconda3\\lib\\site-packages (from comet_ml) (1.16.0)\n",
      "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in c:\\users\\tlotl\\anaconda3\\lib\\site-packages (from comet_ml) (3.2.0)\n",
      "Collecting dulwich!=0.20.33,>=0.20.6\n",
      "  Downloading dulwich-0.20.43-cp39-cp39-win_amd64.whl (496 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.2 in c:\\users\\tlotl\\anaconda3\\lib\\site-packages (from comet_ml) (1.12.1)\n",
      "Requirement already satisfied: requests>=2.18.4 in c:\\users\\tlotl\\anaconda3\\lib\\site-packages (from comet_ml) (2.26.0)\n",
      "Collecting everett[ini]>=1.0.1\n",
      "  Downloading everett-3.0.0-py2.py3-none-any.whl (35 kB)\n",
      "Collecting wurlitzer>=1.0.2\n",
      "  Downloading wurlitzer-3.0.2-py3-none-any.whl (7.3 kB)\n",
      "Collecting nvidia-ml-py3>=7.352.0\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "Collecting requests-toolbelt>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.1 in c:\\users\\tlotl\\anaconda3\\lib\\site-packages (from dulwich!=0.20.33,>=0.20.6->comet_ml) (1.26.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\tlotl\\anaconda3\\lib\\site-packages (from dulwich!=0.20.33,>=0.20.6->comet_ml) (2021.10.8)\n",
      "Collecting configobj\n",
      "  Downloading configobj-5.0.6.tar.gz (33 kB)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\tlotl\\anaconda3\\lib\\site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.18.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tlotl\\anaconda3\\lib\\site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (58.0.4)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\tlotl\\anaconda3\\lib\\site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (21.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tlotl\\anaconda3\\lib\\site-packages (from requests>=2.18.4->comet_ml) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\tlotl\\anaconda3\\lib\\site-packages (from requests>=2.18.4->comet_ml) (2.0.4)\n",
      "Building wheels for collected packages: nvidia-ml-py3, configobj\n",
      "  Building wheel for nvidia-ml-py3 (setup.py): started\n",
      "  Building wheel for nvidia-ml-py3 (setup.py): finished with status 'done'\n",
      "  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19191 sha256=34fe0ad85a70e2cae1c9a7b5494cd7581d9c6033faaf996ce38bcf0fcfc8aa24\n",
      "  Stored in directory: c:\\users\\tlotl\\appdata\\local\\pip\\cache\\wheels\\f6\\d8\\b0\\15cfd7805d39250ac29318105f09b1750683387630d68423e1\n",
      "  Building wheel for configobj (setup.py): started\n",
      "  Building wheel for configobj (setup.py): finished with status 'done'\n",
      "  Created wheel for configobj: filename=configobj-5.0.6-py3-none-any.whl size=34546 sha256=f8850ed552c12dbaaaca2c6645387afd512343e17fe5866adf31646cd831e15f\n",
      "  Stored in directory: c:\\users\\tlotl\\appdata\\local\\pip\\cache\\wheels\\4b\\35\\53\\dfa4d3a4196794cb0a777a97c68dcf02b073d33de9c135d72a\n",
      "Successfully built nvidia-ml-py3 configobj\n",
      "Installing collected packages: everett, configobj, wurlitzer, websocket-client, sentry-sdk, semantic-version, requests-toolbelt, nvidia-ml-py3, dulwich, comet-ml\n",
      "Successfully installed comet-ml-3.31.4 configobj-5.0.6 dulwich-0.20.43 everett-3.0.0 nvidia-ml-py3-7.352.0 requests-toolbelt-0.9.1 semantic-version-2.10.0 sentry-sdk-1.5.12 websocket-client-1.3.3 wurlitzer-3.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install comet_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed07070",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "## 2. Loading the Data\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Loading the data ⚡ |\n",
    "| :--------------------------- |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eef35b6",
   "metadata": {},
   "source": [
    "The training and testing data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "238720ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data\n",
    "\n",
    "# Load test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b588ce7c",
   "metadata": {},
   "source": [
    "<a id=\"three\"></a>\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Exploratory data analysis ⚡ |\n",
    "| :--------------------------- |\n",
    "|This phase is important. This will help to understand patterns in the data, pinpoint any outliers and indicate relationships between variables uusing  descriptive statistics and data visualisations\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f04ce8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e54b645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3203bf60",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "## 4. Data cleaning\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Data Cleaning ⚡ |\n",
    "| :--------------------------- |\n",
    "|  clean the dataset, and possibly create new features -using Natural language process . |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4849949f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a86ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86a6be65",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "## 5. Modelling\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Modelling ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, create one or more regression models that are able to accurately predict the Sentiment. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f829c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5592c0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66154507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddc1c80f",
   "metadata": {},
   "source": [
    "<a id=\"seven\"></a>\n",
    "## 7. Model Explanations\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model explanation ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to discuss how the best performing model works in a simple way so that both technical and non-technical stakeholders can grasp the intuition behind the model's inner workings. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3513b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6614247",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "04a0a30b656975470193f1164805fa6333458e1cf673f69b1bfe1031195afb7d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "989ed68f59dc3d4b7ad13750657e6e5f52210c86bd2d09ccbab3794c2512fbfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
